## 国科大-智能计算系统期末试题（2024春）
填空题
简答题
最后一道大题

[toc]

## 部分题目记录
### 填空题
1. 卷积层中，input维度为16322020，filter维度为1283233，stride=2，pad_left = pad_top = 0,pad_right = pad_bottom = 1,output维度为__（维度顺序定义为n * c * h * w)
2. pytorch中用____函数来构建二维卷积神经网络
3. 在智能计算系统的抽象架构中，设备端版卡级的存储资源为__，芯片级的存储资源为__，cluster级的存储资源为__，core级的存储资源为__
### 简答题
4. 简述单核深度学习处理器(DLP)的片内存储，访存行为以及指令解码过程和传统CPU上的区别
5. 在大模型系统软件中，稀疏注意力机制和FlashAttention机制能够对大模型进行优化，他们有什么区别和联系
6. 胖树（Fat tree）、Dragonfly 和 3D的主要内容和优缺点
### 最后一道大题
7. 假设DLP芯片中有4个cluster，每个cluster中有4个core，host端启动一个任务类型为UNION1，任务规模为 [4,1,2]的任务，请在下表中填写Kernel程序执行时读取到的并行变量的值

| 变量 | 值|
| ---|---|
| taskId | |
|taskIdX ||
|taskIdY ||
|taskIdZ ||
|taskDimX ||
|taskDimY ||
|taskDimZ ||
|taskDim ||
|coreId ||
|coreDim ||
|clusterId ||
|clusterDim	||

## 题解
### 1. 求卷积后的维度

```txt
输出维度 = [N, C_out, H_out, W_out]

其中：
- N：批量大小（保持不变）
- C_out：输出通道数 = 卷积核数量
- H_out：输出高度 = floor((H_in + pad_top + pad_bottom - kernel_height) / stride) + 1
- W_out：输出宽度 = floor((W_in + pad_left + pad_right - kernel_width) / stride) + 1
```

### 2. pytorch里的二维卷积函数

```txt
nn.Conv2d
```

### 3. 智能计算系统存储资源层级

```txt
答案填充：
设备端版卡级的存储资源为 GDDR（全局显存）
芯片级的存储资源为 L2 缓存
cluster 级的存储资源为 共享内存（Shared Memory）
core 级的存储资源为 寄存器（Registers）
各层级存储资源详解：
设备端版卡级（Device Level）
存储类型：GDDR（Graphics Double Data Rate）
特点：
位于版卡（如 GPU 卡）上的大容量内存
带宽高（如 H100 的 GDDR6 带宽达 2TB/s），但延迟较高（约 200ns）
属于设备级全局存储，可被所有芯片访问
典型场景：存储输入数据、模型参数等大规模数据
芯片级（Chip Level）
存储类型：L2 缓存（Level 2 Cache）
特点：
集成在芯片内部的高速缓存
容量通常为数十 MB（如 A100 的 L2 Cache 为 40MB）
带宽和延迟介于 GDDR 与共享内存之间
典型场景：缓存频繁访问的数据，减少 GDDR 访问开销
Cluster 级（Cluster Level）
存储类型：共享内存（Shared Memory）
特点：
属于芯片内某个计算集群（Cluster）的本地存储
容量较小（如 GPU 中每个 SM 的共享内存为 96KB）
带宽极高（接近计算单元速度），延迟极低（<10ns）
典型场景：在集群内的多个核心间共享中间计算结果
Core 级（Core Level）
存储类型：寄存器（Registers）
特点：
每个计算核心私有的极高速存储单元
容量极小（每个 core 通常有数百个寄存器）
访问延迟最低（1-2 个时钟周期）
典型场景：存储当前计算任务的临时变量和操作数
存储层级对比表：
层级	存储类型	容量范围	带宽	延迟	访问范围
版卡级	GDDR	16GB-128GB	1-2TB/s	200-300ns	全设备共享
芯片级	L2 缓存	10MB-50MB	10-50TB/s	50-100ns	单芯片内共享
Cluster 级	共享内存	64KB-256KB	50-100TB/s	10-20ns	集群内核心共享
Core 级	寄存器	数十 KB	100+TB/s	<10ns	单核心私有
关键逻辑说明：
存储层级设计原则：越靠近计算单元（Core）的存储，容量越小但速度越快，形成 "金字塔" 型存储结构
术语对应关系：
"版卡级" 对应设备级全局存储（GDDR）
"芯片级" 对应芯片内的二级缓存（L2 Cache）
"Cluster 级" 对应计算集群内的共享内存（如 GPU 的 Shared Memory）
"Core 级" 对应计算核心私有的寄存器
典型架构映射：该模型与 NVIDIA GPU 架构高度吻合（GDDR→L2 Cache→Shared Memory→Registers）

这种分层存储设计是智能计算系统实现高性能计算的关键，通过多级存储的协同工作，平衡了计算速度与存储容量的矛盾。

```

### 4. DLP 处理器与传统 CPU 的区别

```txt
一、片内存储差异
存储架构
DLP（深度学习处理器）：
采用层次化 SRAM设计，包含共享内存（Shared Memory）和寄存器文件（Register File）。
例如，NVIDIA GPU 的 SM（Streaming Multiprocessor）包含 96KB 共享内存和数千个寄存器，专为矩阵运算优化。
传统 CPU：
采用 ** 多级缓存（L1/L2/L3）** 结构，容量更大（如 L3 可达数十 MB），但带宽低于 DLP 的 SRAM。
容量与带宽对比
DLP：
共享内存带宽通常 > 100TB/s，寄存器带宽接近计算单元速度。
容量较小（共享内存通常 < 1MB），需频繁从外部 DRAM 获取数据。
CPU：
L1 缓存带宽约 50-100TB/s，但 L3 缓存带宽降至 10-20TB/s。
整体容量更大（如 Intel Xeon 的 L3 可达 30MB 以上），适合通用计算。
优化目标
DLP：通过高带宽 SRAM 加速矩阵乘法（如 GEMM 运算）。
CPU：通过大容量缓存减少访存延迟，支持多样化负载。
二、访存行为差异
数据访问模式
DLP：
采用批量并行访问（如 Tensor Core 同时处理 8×8×8 矩阵块）。
支持分块策略（Tile-based），将大矩阵分块存入共享内存。
CPU：
支持随机访问，优化指令级并行（ILP）。
依赖预取机制（Prefetching）减少缓存未命中。
内存层次访问频率
DLP：
优先访问寄存器和共享内存，DRAM 访问占比 > 90%。
例如，NVIDIA Ampere 架构通过 Tensor Cores 实现计算与访存重叠。
CPU：
更均衡地使用各级缓存，L3 缓存命中率较高（如 70%-90%）。
典型优化技术
DLP：
张量核（Tensor Core）：将矩阵乘法分解为更小的块（如 8×8×8）。
内存合并（Memory Coalescing）：相邻线程访问连续内存地址。
CPU：
向量化指令（AVX/AVX512）：单指令处理多个数据。
预取指令（PREFETCH）：提前加载数据到缓存。
三、指令解码差异
指令集架构
DLP：
采用精简指令集（RISC-like），专注于计算密集型指令（如 FP32/FP16/INT8 乘法）。
例如，NVIDIA PTX 指令集包含mma.sync（矩阵乘法累加）指令。
CPU：
支持复杂指令集（CISC/RISC），包含大量控制流和内存操作指令。
解码流程
DLP：
简化的指令解码单元，直接将指令映射到计算单元（如 Tensor Core）。
通常不支持乱序执行，依赖数据并行性。
CPU：
复杂的解码流水线，包含分支预测、乱序执行和寄存器重命名。
并行执行模型
DLP：
采用 **SIMT（单指令多线程）** 架构，数千个线程同时执行相同指令。
例如，GPU 的一个 warp（32 线程）共享同一条指令流。
CPU：
基于 **SMT（超线程）** 或多核并行，每个核心独立执行不同指令流。
四、典型架构对比
特性	DLP（如 NVIDIA GPU）	传统 CPU（如 Intel Xeon）
存储结构	层次化 SRAM（共享内存 + 寄存器）	多级缓存（L1/L2/L3）
带宽（GB/s）	共享内存 > 100,000	L3 缓存～10,000
访存模式	批量并行访问	随机访问 + 预取
指令集	计算密集型指令（如矩阵乘法）	通用指令集（含控制流和内存操作）
并行模型	SIMT（数千线程同步执行）	SMT / 多核（线程 / 核心独立执行）
应用场景	深度学习训练 / 推理、科学计算	通用计算、数据中心负载
五、总结
DLP通过高带宽存储和计算密集型指令优化深度学习负载，牺牲了通用性以换取极致性能。
CPU通过灵活的缓存策略和复杂指令集支持多样化任务，适合需要频繁分支和随机访问的场景。

这种差异源于两者的设计目标：DLP 为专用计算而生，CPU 为通用灵活性设计。
```

### 5. 稀疏注意力与 FlashAttention 的区别和联系

#### **一、核心联系**

1. **共同目标**
   - 均旨在优化 Transformer 模型中的**注意力计算**（Attention Mechanism），降低其时间和空间复杂度。
   - 注意力计算的标准公式为：\(Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)，其中\(QK^T\)的计算复杂度为\(O(n^2)\)（n为序列长度）。
2. **技术定位**
   - 两者是互补而非互斥的优化方向：
     - **稀疏注意力**解决计算复杂度问题（减少\(n^2\)次计算）。
     - **FlashAttention**解决内存访问效率问题（优化\(O(n^2)\)次访问的带宽瓶颈）。

#### **二、主要区别**

1. **优化切入点**
   - 稀疏注意力（Sparse Attention）：
     - 通过**结构化或自适应掩码**跳过部分注意力计算，将复杂度从\(O(n^2)\)降至\(O(n \cdot k)\)（k为稀疏度）。
     - 例如：
       - **固定模式稀疏**：Longformer 的滑动窗口注意力。
       - **自适应稀疏**：Sparse Transformer 根据注意力权重动态选择 top-k 位置。
   - FlashAttention：
     - 基于**分块矩阵乘法**和**重排序算法**，优化内存访问模式，将带宽需求从\(O(n^2)\)降至\(O(n \cdot \sqrt{n})\)。
     - 核心创新：**分块计算 + 原地更新**，减少中间结果存储。
2. **硬件依赖**
   - 稀疏注意力：
     - 高度依赖**专用硬件支持**（如稀疏矩阵加速单元）。
     - 通用 GPU 对稀疏计算的支持有限，需额外软件优化（如 cuSPARSE 库）。
   - FlashAttention：
     - 硬件无关，通过**算法优化**利用现有 GPU 的内存层次结构。
     - 例如，NVIDIA GPU 的共享内存（Shared Memory）和 Tensor Cores 可直接加速分块计算。
3. **实现复杂度**
   - 稀疏注意力：
     - 需设计**稀疏模式**和**高效的稀疏矩阵乘法算法**，实现复杂。
     - 例如，BigBird 的 Block Sparse Attention 需预处理邻接矩阵。
   - FlashAttention：
     - 算法实现更简洁，基于标准矩阵乘法原语（如 cuBLAS）。
     - 核心逻辑：分块计算\(QK^T\)并立即应用 softmax 和V，避免存储完整注意力矩阵。
4. **效果对比**
   - 稀疏注意力：
     - **理论上限高**：理想稀疏度下可将复杂度降至线性\(O(n)\)。
     - **实际收益波动大**：稀疏模式设计不当可能导致精度损失（如重要依赖关系被忽略）。
   - FlashAttention：
     - **稳定加速**：在各种序列长度下均能提升性能（通常 3-5 倍）。
     - **精度无损**：数学等价于标准注意力，仅优化实现方式。

#### **三、典型应用场景**

| **场景**                   | **稀疏注意力**                 | **FlashAttention**       |
| -------------------------- | ------------------------------ | ------------------------ |
| **超长序列（\(n>10^4\)）** | 首选（如文档理解、长文本生成） | 次选（但仍有收益）       |
| **通用 Transformer 优化**  | 需谨慎（可能牺牲精度）         | 推荐（无精度损失）       |
| **硬件限制**               | 需要专用稀疏计算单元           | 适配任何支持 CUDA 的 GPU |
| **模型压缩需求**           | 可结合剪枝减少参数量           | 不直接影响参数量         |

#### **四、结合策略**

两者可协同工作：

1. **稀疏 FlashAttention**：
   - 先用稀疏掩码过滤\(QK^T\)中的非关键位置，再用 FlashAttention 优化剩余计算。
   - 例如：将 Longformer 的滑动窗口与 FlashAttention 结合，同时减少计算量和内存访问。
2. **混合精度支持**：
   - FlashAttention 天然支持 FP16/BF16，稀疏注意力可通过量化进一步压缩计算成本。

#### **五、总结**

- **稀疏注意力**是**计算复杂度**的优化，适合处理极长序列，但依赖特定硬件和模式设计。
- **FlashAttention**是**内存效率**的优化，普适性更强，无需修改模型架构即可提升性能。
- 实际应用中，两者常结合使用，共同推动大模型在资源受限环境下的部署。

### 6. 胖树 (Fat Tree)、Dragonfly 和 3D 网络拓扑

#### **一、胖树 (Fat Tree) 拓扑**

1. **核心结构**
   - **层次化设计**：由**核心层**、**聚合层**和**接入层**组成，形成树形网络。
   - **特点**：上层节点（核心交换机）的链路带宽大于下层节点（边缘交换机），因此称为 "胖树"。
   - 示例：
     - 4-ary 胖树：每个边缘交换机连接 4 个服务器，每个聚合交换机连接 4 个边缘交换机，核心交换机数量为 4²=16 个。
2. **优缺点**
   - 优点：
     - **实现简单**：拓扑结构清晰，路由算法易于设计（如 ECMP，Equal-Cost Multi-Path）。
     - **扩展性好**：通过增加层次或每层节点数可扩展规模。
   - 缺点：
     - **网络利用率低**：上层链路易成为瓶颈（如大量流量汇聚到核心层）。
     - **故障恢复能力弱**：单链路故障可能导致部分路径不可用。
3. **典型应用**
   - 早期数据中心网络（如 Google 的 B4 网络早期版本）。

#### **二、Dragonfly 拓扑**

1. **核心结构**
   - **分组互连**：将网络划分为多个**机柜组（Pod）**，每个机柜组内全连接，机柜组间通过**全局链路**连接。
   - 特点：
     - 机柜组内使用低延迟的短链路，组间使用高带宽的长链路。
     - 路由采用**两阶段策略**：先在组内路由，再跨组路由。
   - 示例：
     - 32 组 ×32 节点的 Dragonfly：每个组内 32 个节点全连接，组间通过稀疏的全局链路连接，总链路数远少于全连接网络。
2. **优缺点**
   - 优点：
     - **低延迟**：大部分通信在组内完成，减少长距离传输。
     - **高带宽**：组内全连接提供高本地带宽，组间链路按需分配。
     - **故障恢复能力强**：多路径冗余，单点故障不影响全局连通性。
   - 缺点：
     - **路由算法复杂**：需同时考虑组内和组间路径选择。
     - **实现成本高**：节点需支持复杂的路由逻辑。
3. **典型应用**
   - 高性能计算集群（如 Cray XC 系列超级计算机）。

#### **三、3D 拓扑**

1. **核心结构**
   - **三维网格或环网**：将节点排列成三维空间结构，节点间通过三个维度的链路连接。
   - 变种：
     - **3D Mesh**：每个节点连接 6 个邻居（三维方向各 2 个）。
     - **3D Torus**：在 3D Mesh 基础上增加环形连接，消除边界。
   - 示例：
     - 8×8×8 的 3D Torus：共 512 个节点，每个节点有 6 条链路，总链路数为 1536 条。
2. **优缺点**
   - 优点：
     - **高带宽密度**：每个节点的链路数固定（如 6 条），扩展性好。
     - **低功耗**：链路长度短，减少信号衰减和能耗。
     - **对称性强**：所有节点地位均等，路由算法简化。
   - 缺点：
     - **物理实现复杂**：三维布线困难，需多层 PCB 或硅通孔（TSV）技术。
     - **故障恢复有限**：单链路故障可能增加路径长度。
3. **典型应用**
   - 片上网络（NoC，Network-on-Chip）设计。
   - 高性能 ASIC/FPGA 互连（如 Google TPU 的片上拓扑）。

#### **四、三种拓扑对比表**

| **特性**     | **胖树 (Fat Tree)**  | **Dragonfly**         | **3D 拓扑**              |
| ------------ | -------------------- | --------------------- | ------------------------ |
| **结构**     | 层次化树形           | 分组全连接 + 全局链路 | 三维网格 / 环网          |
| **复杂度**   | 低（规则结构）       | 高（分组 + 全局路由） | 中（规则但三维实现复杂） |
| **扩展性**   | 好（增加层次或节点） | 好（增加分组或节点）  | 好（增加三维规模）       |
| **延迟**     | 高（跨层传输）       | 低（组内短链路）      | 中（取决于三维规模）     |
| **带宽**     | 上层易成为瓶颈       | 高（组内全连接）      | 高（每个节点固定链路数） |
| **故障恢复** | 弱（单点故障影响大） | 强（多路径冗余）      | 中（环形连接提供备份）   |
| **典型应用** | 数据中心网络         | 超级计算机集群        | 片上网络（NoC）          |

#### **五、实际应用选择**

- **胖树**：适合规模较小、拓扑规则的数据中心，实现成本低。
- **Dragonfly**：适合对延迟和带宽要求极高的 HPC 场景，如 AI 训练集群。
- **3D 拓扑**：适合芯片级互连，尤其是 ASIC/FPGA 等定制化硬件。

现代大型数据中心常采用混合拓扑（如胖树 + Dragonfly 思想），结合多种拓扑的优势以平衡成本、性能和可扩展性。

### 7. DLP 芯片并行变量计算

#### **一、问题背景**

- **硬件架构**：DLP 芯片包含 4 个 Cluster，每个 Cluster 有 4 个 Core，总共有 16 个 Core。
- 任务信息：
  - 任务类型：UNION1（通常表示跨 Cluster 的并行任务）。
  - 任务规模：[4, 1, 2]，对应 (taskDimX=4, taskDimY=1, taskDimZ=2)。
  - 总任务数：4×1×2 = 8 个。

#### **二、解题思路**

1. **确定任务 ID 分配规则**：

   - 任务 ID 按 Z-Y-X 顺序递增，即：

     ```plaintext
     taskId = taskIdZ * taskDimY * taskDimX + taskIdY * taskDimX + taskIdX
     ```

   - 本题中 taskDimY=1，简化为：

     ```plaintext
     taskId = taskIdZ * 4 + taskIdX
     ```

2. **映射任务到 Cluster 和 Core**：

   - 假设任务按 Cluster 维度平均分配，每个 Cluster 处理 8/4=2 个任务。
   - Core 负责执行任务内的子计算（本题未涉及）。

3. **计算并行变量**：

   - 基于任务分配和硬件架构，推导每个变量的值。

#### **三、变量计算过程**

##### 1. **taskId（任务全局 ID）**

- 总任务数为 8，taskId 范围是 0~7。
- 分配规则：按 Z-Y-X 顺序递增。

##### 2. **taskIdX, taskIdY, taskIdZ（任务局部坐标）**

- 由 taskId 反推：

  ```plaintext
  taskIdZ = taskId // (taskDimY * taskDimX) = taskId // 4
  taskIdY = (taskId % (taskDimY * taskDimX)) // taskDimX = 0 （因taskDimY=1）
  taskIdX = taskId % taskDimX
  ```

- 例如：

  - taskId=0 → (taskIdX=0, taskIdY=0, taskIdZ=0)
  - taskId=5 → (taskIdX=1, taskIdY=0, taskIdZ=1)

##### 3. **taskDimX, taskDimY, taskDimZ（任务维度）**

- 直接来自任务规模 [4, 1, 2]：

  ```plaintext
  taskDimX = 4, taskDimY = 1, taskDimZ = 2
  ```

##### 4. **taskDim（任务总维度）**

- 计算公式：

  ```plaintext
  taskDim = taskDimX * taskDimY * taskDimZ = 4 * 1 * 2 = 8
  ```

##### 5. **coreId（当前 Core 的局部 ID）**

- 假设 Core 按顺序编号 0~3（每个 Cluster 内）。
- 本题未指定任务与 Core 的映射关系，默认取 0。

##### 6. **coreDim（每个 Cluster 的 Core 数量）**

- 硬件架构中每个 Cluster 有 4 个 Core：

  ```plaintext
  coreDim = 4
  ```

##### 7. **clusterId（当前 Cluster 的局部 ID）**

- 任务按 Cluster 平均分配，每个 Cluster 处理 2 个任务：

  ```plaintext
  clusterId = taskId // (taskDim / clusterDim) = taskId // 2
  ```

- 例如：

  - taskId=0 → clusterId=0
  - taskId=3 → clusterId=1

##### 8. **clusterDim（总 Cluster 数量）**

- 硬件架构中有 4 个 Cluster：

  ```plaintext
  clusterDim = 4
  ```

#### **四、完整答案表**

| 变量       | 值   | 计算公式 / 说明                  |
| ---------- | ---- | -------------------------------- |
| taskId     | 0    | 任务全局 ID（假设为第一个任务）  |
| taskIdX    | 0    | taskId % taskDimX = 0 % 4 = 0    |
| taskIdY    | 0    | (taskId % 4) // 4 = 0            |
| taskIdZ    | 0    | taskId // 4 = 0 // 4 = 0         |
| taskDimX   | 4    | 任务 X 维度                      |
| taskDimY   | 1    | 任务 Y 维度                      |
| taskDimZ   | 2    | 任务 Z 维度                      |
| taskDim    | 8    | 4 × 1 × 2 = 8                    |
| coreId     | 0    | 假设为每个 Cluster 的第一个 Core |
| coreDim    | 4    | 每个 Cluster 的 Core 数量        |
| clusterId  | 0    | taskId // 2 = 0 // 2 = 0         |
| clusterDim | 4    | 总 Cluster 数量                  |

#### **五、验证示例（taskId=5 的情况）**

- **taskIdX = 5 % 4 = 1**
- **taskIdY = (5 % 4) // 4 = 0**
- **taskIdZ = 5 // 4 = 1**
- **clusterId = 5 // 2 = 2**

其余变量保持不变。

#### **六、关键点总结**

1. **任务 ID 分配**：按 Z-Y-X 顺序映射全局 ID 到局部坐标。
2. 硬件感知：
   - clusterId 与 taskId 的关系取决于任务分配策略。
   - 本题假设任务均匀分布在 Cluster 上，每个 Cluster 处理 2 个任务。
3. 维度一致性：
   - taskDim 需等于 taskDimX × taskDimY × taskDimZ。
   - 总计算单元数（16 Core）大于任务数（8），存在资源冗余。

这种并行变量的设计确保了任务在分布式硬件上的高效调度和执行。